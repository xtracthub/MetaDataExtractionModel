{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, csv, pandas\n",
    "import numpy as np\n",
    "from classDeclarations import file_data\n",
    "from random import sample\n",
    "MAX_SAMPLE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/cc/.local/lib/python3.8/site-packages/pandas/core/strings/accessor.py:101: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "Path = \"../../CDIACPub8\"\n",
    "\n",
    "imgs = []\n",
    "csvs = []\n",
    "pdfs = []\n",
    "\n",
    "missing_count = 0\n",
    "df = pandas.read_csv('../CDIACFileData/labels/cdiac_naivetruth_processed.csv')\n",
    "for subdir, dirs, files in os.walk(Path):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.abspath(os.path.join(subdir, file_name))\n",
    "        if df['path'].str.contains(file_name).any():\n",
    "            if file_path.endswith('.png') or file_path.endswith('.jpg'):\n",
    "                imgs.append(file_path)\n",
    "            elif file_path.endswith('.csv') or file_path.endswith('.tsv'):\n",
    "                csvs.append(file_path)\n",
    "            elif file_path.endswith('.pdf') or file_path.endswith('.txt'):\n",
    "                pdfs.append(file_path)\n",
    "        else:\n",
    "            missing_count += 1\n",
    "\n",
    "\n",
    "img_sample_names = sample(imgs, MAX_SAMPLE_SIZE)\n",
    "csv_sample_names = sample(csvs, MAX_SAMPLE_SIZE)\n",
    "pdfs_sample_names = sample(pdfs, MAX_SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading files now...\n",
      "loading files done!\n"
     ]
    }
   ],
   "source": [
    "print(\"loading files now...\")\n",
    "one_gram = json.load(open('../CDIACFileData/one_grams_distr_256.json', 'r'))\n",
    "two_gram = json.load(open('../CDIACFileData/two_grams_distr_256.json', 'r'))\n",
    "\n",
    "byte_distr = pandas.DataFrame.from_dict(one_gram)\n",
    "two_grams_dicts = pandas.DataFrame.from_dict(two_gram)\n",
    "#best_extractors = json.load(open(\"../CorrelatingExtractors/best_extractors.json\"))\n",
    "print(\"loading files done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_distr_new_index = byte_distr.index.tolist()\n",
    "byte_distr_new_index.append(\"file_type\")\n",
    "byte_distr_new_index.append(\"file_size\")\n",
    "byte_distr = byte_distr.reindex(byte_distr_new_index)\n",
    "\n",
    "two_grams_dicts_new_index = two_grams_dicts.index.tolist()\n",
    "two_grams_dicts_new_index.append(\"file_type\")\n",
    "two_grams_dicts_new_index.append(\"file_size\")\n",
    "two_grams_dicts = two_grams_dicts.reindex(two_grams_dicts_new_index)\n",
    "\n",
    "#byte_distr.sort_index(key=lambda x: x.str.lower())\n",
    "#two_grams_dicts.sort_index(key=lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done with index 0\nDone with index 1\nDone with index 2\n"
     ]
    }
   ],
   "source": [
    "img_data = []\n",
    "csv_data = []\n",
    "pdf_data = []\n",
    "dataset = [img_data, csv_data, pdf_data]\n",
    "for idx, data_list in enumerate(dataset):\n",
    "    curr_sample = []\n",
    "    if idx == 0:\n",
    "        curr_sample = img_sample_names\n",
    "    elif idx == 1:\n",
    "        curr_sample = csv_sample_names\n",
    "    else:\n",
    "        curr_sample = pdfs_sample_names\n",
    "    print(\"Done with index\", idx)\n",
    "\n",
    "    \n",
    "\n",
    "    for file_name in curr_sample:\n",
    "        #best_extractor_row_index = df.path[df.path==file_name].index.tolist()\n",
    "\n",
    "        if  file_name.endswith('.txt') or file_name.endswith('.pdf'):\n",
    "            best_extractor = 'freetext'\n",
    "            byte_distr.at['file_type', file_name] = 0 \n",
    "            two_grams_dicts.at['file_type', file_name] = 0\n",
    "        elif file_name.endswith('.csv') or file_name.endswith('.tsv'):\n",
    "            best_extractor = 'tabular'\n",
    "            byte_distr.at['file_type', file_name] = 1\n",
    "            two_grams_dicts.at['file_type', file_name] = 1\n",
    "        elif file_name.endswith('.png') or file_name.endswith('.jpg'):\n",
    "            best_extractor = 'image'\n",
    "            byte_distr.at['file_type', file_name] = 2\n",
    "            two_grams_dicts.at['file_type', file_name] = 2\n",
    "          \n",
    "        byte_distr.at['file_size', file_name] = os.path.getsize(file_name)\n",
    "        two_grams_dicts.at['file_size', file_name] = os.path.getsize(file_name)\n",
    "\n",
    "        curr_file_data = file_data(file_name, byte_distr[file_name], two_grams_dicts[file_name], best_extractor)\n",
    "        data_list.append(curr_file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dumping!\n",
      "Dumped.\n"
     ]
    }
   ],
   "source": [
    "print('Dumping!')\n",
    "\n",
    "with open('gathered_data_appened_256KB_v1.pkl', 'wb+') as handle:\n",
    "    pickle.dump(dataset, file=handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('Dumped.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}