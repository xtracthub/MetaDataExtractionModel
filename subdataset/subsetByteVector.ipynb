{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import os, json, pickle, csv, pandas\n",
    "import numpy as np\n",
    "from classDeclarations import file_data\n",
    "from random import sample\n",
    "MAX_SAMPLE_SIZE = 100\n",
    "BYTE_SIZE = 262144"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "file_extensions = []\n",
    "with open(\"../file_extensions.txt\", \"r\") as fp_ext:\n",
    "    file_extensions = [line.rstrip() for line in fp_ext]\n",
    "\n",
    "\n",
    "file_extensions = [''] + [x.lower() for x in file_extensions]\n",
    "print(file_extensions)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '0', '23', 'csv', 'doc', 'ds_store', 'otl', 'pdf', 'txt', 'temporaryitems', 'xls', 'zip', '_hydrochem_data', '_pacificasource', '_readme', '_crossover_plots', 'apdisk', 'avi', 'bmp', 'cdf', 'cfg', 'cfl', 'cid', 'csv', 'csv~', 'dat', 'db', 'dbf', 'des', 'dob', 'doc', 'docx', 'exe', 'fig', 'final', 'for', 'geos', 'gif', 'gmtcommands4', 'gz', 'hob', 'htm', 'html', 'idv', 'info', 'inp', 'inv', 'jnl', 'jpg', 'kmz', 'log', 'lst', 'm', 'mat', 'mp4', 'm~', 'nas', 'nc', 'note', 'pdf', 'png', 'ppt', 'prj', 'prn', 'ps', 'py', 'qc', 'rtf', 'save', 'sec', 'shp', 'shx', 'sta', 'tas', 'tsv', 'txt', 'txt~', 'uni', 'var', 'xls', 'xlsx', 'xml']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "Path = \"../../CDIACPub8\"\n",
    "\n",
    "imgs = []\n",
    "tabular = []\n",
    "freetext = []\n",
    "json_xml = []\n",
    "netcdf = []\n",
    "unknown = []\n",
    "\n",
    "missing_count = 0\n",
    "df = pandas.read_csv('../CDIACFileData/labels/cdiac_naivetruth_processed.csv')\n",
    "\n",
    "\n",
    "labelSeriesObj = df['file_label']\n",
    "pathSeriesObj = df['path']\n",
    "\n",
    "\n",
    "for i in range(labelSeriesObj.size):\n",
    "    if labelSeriesObj[i] == 'image':\n",
    "        imgs.append(pathSeriesObj[i])\n",
    "    elif labelSeriesObj[i] == 'tabular':\n",
    "        tabular.append(pathSeriesObj[i])\n",
    "    elif labelSeriesObj[i] == 'freetext':\n",
    "        freetext.append(pathSeriesObj[i])\n",
    "    elif labelSeriesObj[i] == 'json/xml':\n",
    "        json_xml.append(pathSeriesObj[i])\n",
    "    elif labelSeriesObj[i] == 'netcdf':\n",
    "        netcdf.append(pathSeriesObj[i])\n",
    "    else: \n",
    "        unknown.append(pathSeriesObj[i])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(\"loading files now...\")\n",
    "\n",
    "with open('../CDIACFileData/ByteVectors/byte_vector_dict_{bs}B.pkl'.format(bs=BYTE_SIZE), \"rb\") as fp1:\n",
    "    one_gram = pickle.load(fp1)\n",
    "with open('../CDIACFileData/ByteVectors/byte_vector_dict_{bs}B_2grams.pkl'.format(bs=BYTE_SIZE), \"rb\") as fp2:\n",
    "    two_gram = pickle.load(fp2)\n",
    "\n",
    "print(\"loading files done!\")\n",
    "#byte_distr = pandas.DataFrame.from_dict(one_gram)\n",
    "#two_grams_dicts = pandas.DataFrame.from_dict(two_gram)\n",
    "#best_extractors = json.load(open(\"../CorrelatingExtractors/best_extractors.json\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading files now...\n",
      "loading files done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print(len(one_gram))\n",
    "print(len(two_gram))\n",
    "\n",
    "print(one_gram['/home/cc/CDIACPub8/nov_2001_f.xls'].shape)\n",
    "print(two_gram['/home/cc/CDIACPub8/nov_2001_f.xls'].shape)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20427\n",
      "20427\n",
      "(262144,)\n",
      "(262143,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "'''\n",
    "for filename, vector in one_gram.items():\n",
    "\tname, extension = os.path.splitext(filename) # file_name actually holds file path\n",
    "\textension = extension[1:].lower()\n",
    "\n",
    "\tone_gram[filename] = np.append(one_gram[filename], file_extensions.index(extension))\n",
    "\tone_gram[filename] = np.append(one_gram[filename], os.path.getsize(filename))\n",
    "\n",
    "for filename, vector in two_gram.items():\n",
    "\tname, extension = os.path.splitext(filename) # file_name actually holds file path\n",
    "\textension = extension[1:].lower()\n",
    "\n",
    "\ttwo_gram[filename] = np.append(two_gram[filename], file_extensions.index(extension))\n",
    "\ttwo_gram[filename] = np.append(two_gram[filename], os.path.getsize(filename))\n",
    "'''\n",
    "\t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(len(one_gram))\n",
    "print(len(two_gram))\n",
    "\n",
    "print(one_gram['/home/cc/CDIACPub8/nov_2001_f.xls'].shape)\n",
    "print(two_gram['/home/cc/CDIACPub8/nov_2001_f.xls'].shape)\n",
    "\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20427\n",
      "20427\n",
      "(512,)\n",
      "(511,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def generate_sample(iter):\n",
    "    img_sample_names = sample(imgs, MAX_SAMPLE_SIZE)\n",
    "    tabular_sample_names = sample(tabular, MAX_SAMPLE_SIZE)\n",
    "    freetext_sample_names = sample(freetext, MAX_SAMPLE_SIZE)\n",
    "    json_xml_sample_names = sample(json_xml, MAX_SAMPLE_SIZE)\n",
    "    netcdf_sample_names = sample(netcdf, MAX_SAMPLE_SIZE)\n",
    "    unknown_sample_names = sample(unknown, MAX_SAMPLE_SIZE)\n",
    "\n",
    "    img_data = []\n",
    "    tabular_data = []\n",
    "    freetext_data = []\n",
    "    json_xml_data = []\n",
    "    netcdf_data = []\n",
    "    unknown_data = []\n",
    "    dataset = [img_data, tabular_data, freetext_data, json_xml_data, netcdf_data, unknown_data]\n",
    "\n",
    "    for idx, data_list in enumerate(dataset):\n",
    "        curr_sample = []\n",
    "        if idx == 0:\n",
    "            best_extractor = 'image'\n",
    "            curr_sample = img_sample_names\n",
    "        elif idx == 1:\n",
    "            best_extractor = 'tabular'\n",
    "            curr_sample = tabular_sample_names\n",
    "        elif idx == 2:\n",
    "            best_extractor = 'freetext'\n",
    "            curr_sample = freetext_sample_names\n",
    "        elif idx == 3:\n",
    "            best_extractor = 'json/xml'\n",
    "            curr_sample = json_xml_sample_names\n",
    "        elif idx == 4:\n",
    "            best_extractor = 'netcdf'\n",
    "            curr_sample = netcdf_sample_names\n",
    "        elif idx == 5:\n",
    "            best_extractor = 'unknown'\n",
    "            curr_sample = unknown_sample_names\n",
    "        print(\"Done with index\", idx)\n",
    "\n",
    "        for file_name in curr_sample:\n",
    "            curr_file_data = file_data(file_name, one_gram[file_name], two_gram[file_name].astype(np.uint16), best_extractor)\n",
    "            data_list.append(curr_file_data)\n",
    "\n",
    "        print('Dumping!')\n",
    "\n",
    "        with open('gathered_data_byte_vector_raw/sample{x}/byte_vectors_v{x}_{bs}B.pkl'.format(x=iter, bs=BYTE_SIZE), 'wb+') as handle:\n",
    "            pickle.dump(dataset, file=handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('Dumped.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for i in range(1, 4):\n",
    "\tgenerate_sample(i)\n",
    "\tprint(\"Generated:\", i)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done with index 0\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 1\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 2\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 3\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 4\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 5\n",
      "Dumping!\n",
      "Dumped.\n",
      "Generated: 1\n",
      "Done with index 0\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 1\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 2\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 3\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 4\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 5\n",
      "Dumping!\n",
      "Dumped.\n",
      "Generated: 2\n",
      "Done with index 0\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 1\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 2\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 3\n",
      "Dumping!\n",
      "Dumped.\n",
      "Done with index 4\n",
      "Dumping!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}